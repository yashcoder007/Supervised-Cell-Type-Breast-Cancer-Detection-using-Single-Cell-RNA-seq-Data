{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9606ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif, SelectKBest, f_classif\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load data from csv file\n",
    "data = pd.read_csv(\"datapbmc10xV2NormWithColNamesRowNames.csv\")\n",
    "data.head(10) \n",
    "\n",
    "data.set_index(\"Unnamed: 0\", inplace = True)\n",
    "data.index.name = None\n",
    "\n",
    "X = data.iloc[:, data.columns != 'cellType'].values\n",
    "#X=data.iloc[:,0:4915]\n",
    "X = np.absolute(X)\n",
    "\n",
    "y = data.iloc[:,data.columns == 'cellType'].values.ravel()\n",
    "\n",
    "def precision_MC(y_true, y_pred):\n",
    "    TP = np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FP = confusion_matrix(y_true, y_pred).sum(axis=1) - np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FP = FP.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    precision = TP/(TP+FP) * 100\n",
    "    return precision\n",
    "\n",
    "def recall_MC(y_true, y_pred):\n",
    "    TP = np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FN = confusion_matrix(y_true, y_pred).sum(axis=0) - np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    recall =  TP/(TP+FN) * 100\n",
    "    return recall\n",
    "\n",
    "def accuracy_MC(y_true, y_pred):\n",
    "    FP = confusion_matrix(y_true, y_pred).sum(axis=1) - np.diag(confusion_matrix(y_true, y_pred))  \n",
    "    FN = confusion_matrix(y_true, y_pred).sum(axis=0) - np.diag(confusion_matrix(y_true, y_pred))\n",
    "    TP = np.diag(confusion_matrix(y_true, y_pred))\n",
    "    TP = TP.astype(float)\n",
    "    TN = confusion_matrix(y_true, y_pred).sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN) * 100\n",
    "    return accuracy\n",
    "\n",
    "def npval_MC(y_true, y_pred): \n",
    "    FN = confusion_matrix(y_true, y_pred).sum(axis=0) - np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FN = FN.astype(float)\n",
    "    TP = np.diag(confusion_matrix(y_true, y_pred))\n",
    "    TP = TP.astype(float)\n",
    "    FP = confusion_matrix(y_true, y_pred).sum(axis=1) - np.diag(confusion_matrix(y_true, y_pred)) \n",
    "    FP = FP.astype(float)\n",
    "    TN = confusion_matrix(y_true, y_pred).sum() - (FP + FN + TP)\n",
    "    TN = TN.astype(float)\n",
    "    npv = TN/(TN+FN) * 100\n",
    "    return npv\n",
    "\n",
    "def specificity_MC(y_true, y_pred): \n",
    "    FP = confusion_matrix(y_true, y_pred).sum(axis=1) - np.diag(confusion_matrix(y_true, y_pred)) \n",
    "    FP = FP.astype(float)\n",
    "    FN = confusion_matrix(y_true, y_pred).sum(axis=0) - np.diag(confusion_matrix(y_true, y_pred))\n",
    "    FN = FN.astype(float)\n",
    "    TP = np.diag(confusion_matrix(y_true, y_pred))\n",
    "    TP = TP.astype(float)\n",
    "    TN = confusion_matrix(y_true, y_pred).sum() - (FP + FN + TP)\n",
    "    TN = TN.astype(float)\n",
    "    spec = TN/(TN+FP)  * 100\n",
    "    return spec\n",
    "\n",
    "\n",
    "def plot_ROC_curve_MC(y_true, y_pred, FSName):\n",
    "    n_classes = len(np.unique(y_true))\n",
    "    y_true = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "    #y_pred = label_binarize(y_pred, classes=np.arange(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    #colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            label=\"ROC curve of class {0} (area = {1:0.4f})\".format(i, roc_auc[i]),\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve of \" + str(FSName))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    path = \"BIBM_Workshop/Results_RF_Bayesian/ROC_\"+ str(FSName)\n",
    "    plt.savefig(f'{path}.png')\n",
    "\n",
    "#Bayesian Search\n",
    "bayesian_search = BayesSearchCV(\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'criterion': Categorical(['gini','entropy']),\n",
    "            'max_depth': Integer(1,60),\n",
    "            'n_estimators':(100,500),\n",
    "            'max_features': (['sqrt','log2'])\n",
    "            #'max_leaf_nodes':Integer(0,prior='log-uniform'),\n",
    "            #'min_samples_leaf':Real(0.1,0.5,prior='uniform'),\n",
    "            #'min_samples_split':Real(0.1,1,prior='uniform')\n",
    "        },\n",
    "        n_iter=50,scoring='accuracy',n_jobs = -1, verbose = 10,\n",
    "        random_state=0, cv=10\n",
    "    )\n",
    "\n",
    "#grid_search = GridSearchCV(SVC(), tuned_params, cv=10, scoring='accuracy',n_jobs = -1, verbose = 10)\n",
    "bayesian_search.fit(X, y)\n",
    "bayesian_search.best_params_\n",
    "print(\"\")\n",
    "print(\"Best Parameters Found\")\n",
    "print(bayesian_search.best_params_) \n",
    "clf = RandomForestClassifier(criterion = bayesian_search.best_params_['criterion'], max_depth = bayesian_search.best_params_['max_depth'], n_estimators= bayesian_search.best_params_['n_estimators'],\n",
    "      max_features= bayesian_search.best_params_['max_features'])\n",
    "\n",
    "#clf = SVC(kernel = 'linear', C= 0.1)\n",
    "k_range = [100,200,300,400]\n",
    "acc_chi2 = []\n",
    "acc_mi = []\n",
    "acc_annova = []\n",
    "\n",
    "for i in k_range:    \n",
    "    X_kbest_features_annova = SelectKBest(f_classif, k=i).fit_transform(X, y)\n",
    "    \n",
    "    acc = cross_val_score(clf, X_kbest_features_annova, y, cv=10, scoring=\"accuracy\").mean() \n",
    "    acc_annova.append(acc*100)\n",
    "    print(acc_annova)\n",
    "    \n",
    "    X_kbest_features_chi2 = SelectKBest(chi2, k=i).fit_transform(X, y)\n",
    "    \n",
    "    acc1 = cross_val_score(clf, X_kbest_features_chi2, y, cv=10, scoring=\"accuracy\").mean() \n",
    "    acc_chi2.append(acc1*100)\n",
    "    print(acc_chi2)\n",
    "    \n",
    "    X_kbest_features_mi = SelectKBest(mutual_info_classif, k=i).fit_transform(X,y)\n",
    "    \n",
    "    acc2 = cross_val_score(clf, X_kbest_features_mi, y, cv=10, scoring=\"accuracy\").mean()\n",
    "    acc_mi.append(acc2*100)\n",
    "    print(acc_mi)\n",
    "\n",
    "print(\"Accuracy Anova\", acc_annova)\n",
    "print(\"Chi2 accuracy\", acc_chi2)\n",
    "print(\"IG accuracy\", acc_mi)\n",
    "\n",
    "optimal_k_annova = k_range[acc_annova.index(max(acc_annova))]\n",
    "optimal_k_chi2 = k_range[acc_chi2.index(max(acc_chi2))]\n",
    "optimal_k_mi = k_range[acc_mi.index(max(acc_mi))]\n",
    "\n",
    "# multiple line plot of all Feature selection methods\n",
    "plt.plot( k_range, acc_annova,color='blue', linewidth=2, label = \"Anova F-value\")\n",
    "plt.plot( k_range, acc_chi2, color='orange', linewidth=2, label =\"Chi2\")\n",
    "plt.plot( k_range, acc_mi, color='green', linewidth=2, label=\"Information Gain\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Features k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "plt.savefig('BIBM_Workshop/Results_RF_Bayesian/Bestk.png')\n",
    "\n",
    "data1 = data\n",
    "data1.drop(columns='cellType', inplace = True)\n",
    "data1.head(10)\n",
    "\n",
    "df_columns = pd.DataFrame(data1.columns)\n",
    "\n",
    "#Printing top 20 features_anova\n",
    "bestfeatures_anova = SelectKBest(f_classif, k= optimal_k_annova)\n",
    "fit_anova = bestfeatures_anova.fit(X,y)\n",
    "df_scores_anova = pd.DataFrame(fit_anova.scores_)\n",
    "\n",
    "# concatenate dataframes\n",
    "feature_scores_anova = pd.concat([df_columns, df_scores_anova],axis=1)\n",
    "feature_scores_anova.columns = ['Feature_Name','Score']  # name output columns\n",
    "print(\"Anova\")\n",
    "print(feature_scores_anova.nlargest(20,'Score')) \n",
    "# export selected features to .csv\n",
    "df_feat_anova = feature_scores_anova.nlargest(20,'Score')\n",
    "df_feat_anova.to_csv('/home/horas/sheena/BIBM_Workshop/Results_RF_Bayesian/feature_selected_Anova.csv', index=False)\n",
    "\n",
    "#Printing top 20 features_chi2\n",
    "bestfeatures_chi2 = SelectKBest(chi2, k= optimal_k_chi2)\n",
    "fit_chi2 = bestfeatures_chi2.fit(X,y)\n",
    "df_scores_chi2 = pd.DataFrame(fit_chi2.scores_)\n",
    "\n",
    "# concatenate dataframes\n",
    "feature_scores_chi2 = pd.concat([df_columns, df_scores_chi2],axis=1)\n",
    "feature_scores_chi2.columns = ['Feature_Name','Score']  # name output columns\n",
    "print(\"CHI2\")\n",
    "print(feature_scores_chi2.nlargest(20,'Score')) \n",
    "# export selected features to .csv\n",
    "df_feat_chi2 = feature_scores_chi2.nlargest(20,'Score')\n",
    "df_feat_chi2.to_csv('BIBM_Workshop/Results_RF_Bayesian/feature_selected_Chi2.csv', index=False)\n",
    "\n",
    "#Printing top 20 features_IG\n",
    "bestfeatures_ig = SelectKBest(mutual_info_classif, k= optimal_k_mi)\n",
    "fit_ig = bestfeatures_ig.fit(X,y)\n",
    "df_scores_ig = pd.DataFrame(fit_ig.scores_)\n",
    "\n",
    "# concatenate dataframes\n",
    "feature_scores_ig = pd.concat([df_columns, df_scores_ig],axis=1)\n",
    "feature_scores_ig.columns = ['Feature_Name','Score']  # name output columns\n",
    "print(\"IG\")\n",
    "print(feature_scores_ig.nlargest(20,'Score')) \n",
    "# export selected features to .csv\n",
    "df_feat_ig = feature_scores_ig.nlargest(20,'Score')\n",
    "df_feat_ig.to_csv('BIBM_Workshop/Results_RF_Bayesian/feature_selected_IG.csv', index=False)\n",
    "\n",
    "perf_cols = [\"Cell Type\", \"Accuracy%\",\"Precision%\",\"Recall%\",\"Negative Predictive Value%\",\"Specificity%\",\"F1-score%\"]\n",
    "perf_anova  = pd.DataFrame(columns = perf_cols)\n",
    "perf_chi2  = pd.DataFrame(columns = perf_cols)\n",
    "perf_mi  = pd.DataFrame(columns = perf_cols)\n",
    "\n",
    "label = np.unique(y.tolist())\n",
    "class_lbl = [int(i) for i in label] \n",
    "class_lbl = ['CellType ' + str(s) for s in class_lbl]\n",
    "\n",
    "#Feature selection\n",
    "X_kbestfeatures_annova = SelectKBest(f_classif, k = optimal_k_annova).fit_transform(X, y)\n",
    "X_kbestfeatures_chi2 = SelectKBest(chi2, k = optimal_k_chi2).fit_transform(X, y)\n",
    "X_kbestfeatures_mi = SelectKBest(mutual_info_classif, k = optimal_k_mi).fit_transform(X, y)\n",
    "\n",
    "print(X_kbestfeatures_annova.shape) #(1724, 300)\n",
    "print(X_kbestfeatures_chi2.shape) #(1724, 400)\n",
    "print(X_kbestfeatures_mi.shape) #(1724, 200)\n",
    "\n",
    "#Anova + classification Method\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "X_train_anova, X_test_anova, y_train_anova, y_test_anova = train_test_split(X_kbestfeatures_annova, y, test_size = 0.30, stratify=y, random_state= random_state)\n",
    "clf.fit(X_train_anova, y_train_anova)\n",
    "y_pred_anova = clf.predict(X_test_anova)\n",
    "\n",
    "X_train_anova.shape #(1351,400) , (1206, 300)\n",
    "X_test_anova.shape #(579, 400), (518, 300)\n",
    "\n",
    "anova_accuracy = accuracy_score(y_test_anova, y_pred_anova)\n",
    "print(\"Anova Accuracy\")\n",
    "print(anova_accuracy) #0.9792746113989638, 0.8687258687258688\n",
    "\n",
    "pre_anova = precision_MC(y_test_anova,y_pred_anova)\n",
    "rec_anova = recall_MC(y_test_anova,y_pred_anova)\n",
    "acc_anova = accuracy_MC(y_test_anova,y_pred_anova)\n",
    "npv_anova = npval_MC(y_test_anova,y_pred_anova)\n",
    "spe_anova = specificity_MC(y_test_anova,y_pred_anova)\n",
    "f1_anova = 2* (pre_anova * rec_anova)/(pre_anova + rec_anova)\n",
    "\n",
    "print(\"RF MultiClass and Anova\")\n",
    "print(pre_anova)\n",
    "print(rec_anova)\n",
    "print(acc_anova)\n",
    "print(npv_anova)\n",
    "print(spe_anova)\n",
    "print(f1_anova)\n",
    "\n",
    "perf_anova['Cell Type'] = class_lbl\n",
    "perf_anova['Precision%'] = pre_anova.tolist()\n",
    "perf_anova['Accuracy%'] =  acc_anova.tolist()\n",
    "perf_anova['Recall%']  = rec_anova.tolist()\n",
    "perf_anova['Negative Predictive Value%'] = npv_anova.tolist()\n",
    "perf_anova['Specificity%'] = spe_anova.tolist()\n",
    "perf_anova['F1-score%'] = f1_anova.tolist()\n",
    "\n",
    "perf_anova.to_csv('BIBM_Workshop/Results_RF_Bayesian/PerformanceMetrics_Anova.csv', index = False)\n",
    "\n",
    "#Plot ROC curve for OneVsRest + Anova\n",
    "#y_score_anova = classifier.fit(X_train_anova, y_train_anova).decision_function(X_test_anova)\n",
    "\n",
    "n_classes = len(np.unique(y_test_anova))\n",
    "y_pred_an = label_binarize(y_pred_anova, classes=np.arange(n_classes))\n",
    "\n",
    "plot_ROC_curve_MC(y_test_anova, y_pred_an, \"Anova\")\n",
    "\n",
    "#Chi2 + classification Method\n",
    "X_train_chi2, X_test_chi2, y_train_chi2, y_test_chi2 = train_test_split(X_kbestfeatures_chi2, y, test_size = 0.30, stratify=y, random_state= random_state)\n",
    "clf.fit(X_train_chi2, y_train_chi2)\n",
    "y_pred_chi2 = clf.predict(X_test_chi2)\n",
    "\n",
    "X_train_anova.shape #(1351,400), (1206, 400)\n",
    "X_test_anova.shape #(579, 400), (518, 400)\n",
    "\n",
    "chi2_accuracy = accuracy_score(y_test_chi2, y_pred_chi2)\n",
    "print(\"Chi2 Accuracy\")\n",
    "print(chi2_accuracy) #0.9585492227979274, 0.6621621621621622\n",
    "\n",
    "pre_chi2 = precision_MC(y_test_chi2,y_pred_chi2)\n",
    "rec_chi2 = recall_MC(y_test_chi2,y_pred_chi2)\n",
    "acc_chi2 = accuracy_MC(y_test_chi2,y_pred_chi2)\n",
    "npv_chi2 = npval_MC(y_test_chi2,y_pred_chi2)\n",
    "spe_chi2 = specificity_MC(y_test_chi2,y_pred_chi2)\n",
    "f1_chi2 = 2* (pre_chi2 * rec_chi2)/(pre_chi2 + rec_chi2)\n",
    "\n",
    "print(\"SVC MultiClass and Chi2\")\n",
    "print(pre_chi2)\n",
    "print(rec_chi2)\n",
    "print(acc_chi2)\n",
    "print(npv_chi2)\n",
    "print(spe_chi2)\n",
    "print(f1_chi2)\n",
    "\n",
    "perf_chi2['Cell Type'] = class_lbl\n",
    "perf_chi2['Precision%'] = pre_chi2.tolist()\n",
    "perf_chi2['Accuracy%'] =  acc_chi2.tolist()\n",
    "perf_chi2['Recall%']  = rec_chi2.tolist()\n",
    "perf_chi2['Negative Predictive Value%'] = npv_chi2.tolist()\n",
    "perf_chi2['Specificity%'] = spe_chi2.tolist()\n",
    "perf_chi2['F1-score%'] = f1_chi2.tolist()\n",
    "\n",
    "perf_chi2.to_csv('BIBM_Workshop/Results_RF_Bayesian/PerformanceMetrics_Chi2.csv', index = False)\n",
    "\n",
    "#Plot ROC curve for OneVsRest + Chi2\n",
    "#y_score_chi2 = classifier.fit(X_train_chi2, y_train_chi2).decision_function(X_test_chi2)\n",
    "\n",
    "n_classes = len(np.unique(y_test_chi2))\n",
    "y_pred_chi = label_binarize(y_pred_chi2, classes=np.arange(n_classes))\n",
    "\n",
    "plot_ROC_curve_MC(y_test_chi2, y_pred_chi, \"Chi2\")\n",
    "\n",
    "#IG + classification Method\n",
    "X_train_mi, X_test_mi, y_train_mi, y_test_mi = train_test_split(X_kbestfeatures_mi, y, test_size = 0.30, stratify=y, random_state= random_state)\n",
    "clf.fit(X_train_mi, y_train_mi)\n",
    "y_pred_mi = clf.predict(X_test_mi)\n",
    "\n",
    "X_train_mi.shape #(1351,400), (1206, 400)\n",
    "X_test_mi.shape #(579, 400), (518, 400)\n",
    "\n",
    "mi_accuracy = accuracy_score(y_test_mi, y_pred_mi)\n",
    "print(\"IG Accuracy\")\n",
    "print(mi_accuracy) #0.9775474956822107, 0.972972972972973\n",
    "\n",
    "pre_mi = precision_MC(y_test_mi,y_pred_mi)\n",
    "rec_mi = recall_MC(y_test_mi,y_pred_mi)\n",
    "acc_mi = accuracy_MC(y_test_mi,y_pred_mi)\n",
    "npv_mi = npval_MC(y_test_mi,y_pred_mi)\n",
    "spe_mi = specificity_MC(y_test_mi,y_pred_mi)\n",
    "f1_mi = 2* (pre_mi * rec_mi)/(pre_mi + rec_mi)\n",
    "\n",
    "print(\"SVC MultiClass and IG\")\n",
    "print(pre_mi)\n",
    "print(rec_mi)\n",
    "print(acc_mi)\n",
    "print(npv_mi)\n",
    "print(spe_mi)\n",
    "print(f1_mi)\n",
    "\n",
    "perf_mi['Cell Type'] = class_lbl\n",
    "perf_mi['Precision%'] = pre_mi.tolist()\n",
    "perf_mi['Accuracy%'] =  acc_mi.tolist()\n",
    "perf_mi['Recall%']  = rec_mi.tolist()\n",
    "perf_mi['Negative Predictive Value%'] = npv_mi.tolist()\n",
    "perf_mi['Specificity%'] = spe_mi.tolist()\n",
    "perf_mi['F1-score%'] = f1_mi.tolist()\n",
    "\n",
    "perf_mi.to_csv('BIBM_Workshop/Results_RF_Bayesian/PerformanceMetrics_IG.csv', index = False)\n",
    "\n",
    "#Plot ROC curve for OneVsRest + IG\n",
    "#y_score_mi = classifier.fit(X_train_mi, y_train_mi).decision_function(X_test_mi)\n",
    "\n",
    "n_classes = len(np.unique(y_test_mi))\n",
    "y_pred_ig = label_binarize(y_pred_mi, classes=np.arange(n_classes))\n",
    "\n",
    "plot_ROC_curve_MC(y_test_mi, y_pred_ig, \"IG\")\n",
    "\n",
    "## Tabular Representation of Efficiency\n",
    "table11= plt.table(cellText=perf_anova.values,\n",
    "          colLabels=perf_anova.columns,\n",
    "          cellLoc = 'center', rowLoc = 'center', colWidths=[0.25 for x in perf_anova.columns],\n",
    "          loc='center')\n",
    "table11.auto_set_font_size(False)\n",
    "table11.set_fontsize(10)\n",
    "table11.scale(2, 2)\n",
    "plt.title('Results - RF with Anova', pad = '80.0')\n",
    "plt.axis('off')\n",
    "\n",
    "## Tabular Representation of Efficiency\n",
    "table22= plt.table(cellText=perf_chi2.values,\n",
    "          colLabels=perf_chi2.columns,\n",
    "          cellLoc = 'center', rowLoc = 'center', colWidths=[0.25 for x in perf_chi2.columns],\n",
    "          loc='center')\n",
    "table22.auto_set_font_size(False)\n",
    "table22.set_fontsize(10)\n",
    "table22.scale(2, 2)\n",
    "plt.title('Results - RF with Chi2', pad = '80.0')\n",
    "plt.axis('off')\n",
    "\n",
    "## Tabular Representation of Efficiency\n",
    "table33= plt.table(cellText=perf_mi.values,\n",
    "          colLabels=perf_mi.columns,\n",
    "          cellLoc = 'center', rowLoc = 'center', colWidths=[0.25 for x in perf_mi.columns],\n",
    "          loc='center')\n",
    "table33.auto_set_font_size(False)\n",
    "table33.set_fontsize(10)\n",
    "table33.scale(2, 2)\n",
    "plt.title('Results - RF with Information Gain', pad = '80.0')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
